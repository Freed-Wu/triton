{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Persistent Matmul\nThis script demonstrates persistent kernel implementations of matrix multiplication using Triton.\nVarious matmul methods are included, such as naive, persistent, and TMA (Tensor Memory Accelerator) based approaches.\nThe kernels support both FP16 and FP8 data types but the FP8 implementation is only available on CUDA devices with compute capability >= 9.0.\n\nTriton and cuBLAS implementations are benchmarked under different configurations and evaluated using the proton profiler.\nUsers can pass command-line arguments to specify matrix dimensions and iteration steps flexibly.\n\n```bash\n# FP8\npython 09-persistent-matmul.py --prec fp8 --K_range 128 1024 --K_step 128\n\n# FP16\npython 09-persistent-matmul.py --prec fp16 --K_range 128 1024 --K_step 128\n```\nNote that currently this tutorial will fail on devices with a small shared memory size, such as RTX-4090.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\n\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.tools.experimental_descriptor\nimport triton.profiler as proton\nfrom contextlib import contextmanager\n\nfrom typing import Optional\n\nif torch.cuda.is_available():\n    from triton._C.libtriton import nvidia\n    cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n    cublas = nvidia.cublas.CublasLt(cublas_workspace)\nelse:\n    cublas = None\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef supports_tma():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\nHAS_TMA_DESC = \"nv_tma_desc_type\" in dir(tl)\n\nif HAS_TMA_DESC:\n    print(\"TMA benchmarks will be running with experimental grid constant TMA descriptor.\", )\nelse:\n    print(\"TMA benchmarks will be running without grid constant TMA descriptor.\", )\n\n\n# TmaAutoTuneHelper used in htyu's PR #5622\nclass TmaAutoTuneHelper:\n\n    # duck typing wrapper to implement the same interface as TmaDescKernelParam in Triton PR #4498\n    class KernelParamWrapper:\n\n        def __init__(self, desc):\n            self.desc = desc\n\n        def tma_desc_cpu_ptr(self):\n            return self.desc.data_ptr()\n\n    TMA_SIZE = 128\n\n    def __init__(self):\n        self.fill_1d_tma_descriptor_inner = (triton.runtime.driver.active.utils.fill_1d_tma_descriptor)\n        self.fill_2d_tma_descriptor_inner = (triton.runtime.driver.active.utils.fill_2d_tma_descriptor)\n        if HAS_TMA_DESC:\n            self.descriptors = {}\n        else:\n            self.cuda_descriptors = {}\n\n    # Call this method outside of the lambda function for grid size\n    def init_tma_descriptor(self, name):\n        if HAS_TMA_DESC:\n            self.descriptors[name] = torch.empty(TmaAutoTuneHelper.TMA_SIZE, device=\"cpu\", dtype=torch.int8)\n        else:\n            self.cuda_descriptors[name] = torch.empty(TmaAutoTuneHelper.TMA_SIZE, device=\"cuda\", dtype=torch.int8)\n\n    # Call this method inside the lambda function for grid size\n    def fill_1d_tma_descriptor(self, name, ptr, dim, block_dim, element_size):\n        if HAS_TMA_DESC:\n            desc_x = self.descriptors[name]\n            assert desc_x.data_ptr() % 64 == 0\n            self.fill_1d_tma_descriptor_inner(ptr, dim, block_dim, element_size, desc_x.data_ptr())\n        else:\n            desc_x = self.cuda_descriptors[name]\n            buf_x = torch.empty_like(desc_x, device=\"cpu\", pin_memory=True)\n            self.fill_1d_tma_descriptor_inner(ptr, dim, block_dim, element_size, buf_x.data_ptr())\n            desc_x.copy_(buf_x, non_blocking=True)\n\n    # Call this method inside the lambda function for grid size\n    def fill_2d_tma_descriptor(self, name, ptr, dim1, dim0, block_dim1, block_dim0, element_size):\n        if HAS_TMA_DESC:\n            desc_x = self.descriptors[name]\n            assert desc_x.data_ptr() % 64 == 0\n            self.fill_2d_tma_descriptor_inner(ptr, dim1, dim0, block_dim1, block_dim0, element_size, desc_x.data_ptr())\n        else:\n            desc_x = self.cuda_descriptors[name]\n            buf_x = torch.empty_like(desc_x, device=\"cpu\", pin_memory=True)\n            self.fill_2d_tma_descriptor_inner(ptr, dim1, dim0, block_dim1, block_dim0, element_size, buf_x.data_ptr())\n            desc_x.copy_(buf_x, non_blocking=True)\n\n    def get_tma_descriptor_kernel_param(self, name):\n        if HAS_TMA_DESC:\n            assert self.descriptors[name] is not None\n            return self.KernelParamWrapper(self.descriptors[name])\n        else:\n            assert self.cuda_descriptors[name] is not None\n            return self.cuda_descriptors[name]\n\n\ndef matmul_get_configs():\n    return [\n        triton.Config({'BLOCK_SIZE_M': BM, 'BLOCK_SIZE_N': BN, \"BLOCK_SIZE_K\" : BK, \"GROUP_SIZE_M\" : 8}, num_stages=s, num_warps=w) \\\n        for BM in [128] \\\n        for BN in [128, 256] \\\n        for BK in [64,128] \\\n        for s in ([3,4]) \\\n        for w in [4,8] \\\n    ]\n\n\n@triton.autotune(\n    configs=matmul_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                  M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  BLOCK_SIZE_M: tl.constexpr,  #\n                  BLOCK_SIZE_N: tl.constexpr,  #\n                  BLOCK_SIZE_K: tl.constexpr,  #\n                  GROUP_SIZE_M: tl.constexpr,  #\n                  ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if (c_ptr.dtype.element_ty == tl.float8e4nv):\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n    matmul_kernel[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n    )\n    return c\n\n\n@triton.autotune(\n    configs=matmul_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr,  #\n                             M, N, K,  #\n                             stride_am, stride_ak,  #\n                             stride_bk, stride_bn,  #\n                             stride_cm, stride_cn,  #\n                             BLOCK_SIZE_M: tl.constexpr,  #\n                             BLOCK_SIZE_N: tl.constexpr,  #\n                             BLOCK_SIZE_K: tl.constexpr,  #\n                             GROUP_SIZE_M: tl.constexpr,  #\n                             NUM_SMS: tl.constexpr,  #\n                             ):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if (c_ptr.dtype.element_ty == tl.float8e4nv):\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_persistent(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n    # Allocates output.\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        NUM_SMS=NUM_SMS,  #\n    )\n    return c\n\n\ndef matmul_tma_persistent_get_configs():\n    return [\n        triton.Config({'BLOCK_SIZE_M': BM, 'BLOCK_SIZE_N': BN, \"BLOCK_SIZE_K\" : BK, \"GROUP_SIZE_M\" : 8, \"EPILOGUE_SUBTILE\" : SUBTILE}, num_stages=s, num_warps=w) \\\n        for BM in [128] \\\n        for BN in [128, 256] \\\n        for BK in [64, 128] \\\n        for s in ([3, 4]) \\\n        for w in [4, 8] \\\n        for SUBTILE in [True, False] \\\n    ]\n\n\n@triton.autotune(\n    configs=matmul_tma_persistent_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_tma_persistent(a_desc_ptr, b_desc_ptr, c_desc_ptr,  #\n                                 M, N, K,  #\n                                 BLOCK_SIZE_M: tl.constexpr,  #\n                                 BLOCK_SIZE_N: tl.constexpr,  #\n                                 BLOCK_SIZE_K: tl.constexpr,  #\n                                 GROUP_SIZE_M: tl.constexpr,  #\n                                 FP8_OUTPUT: tl.constexpr,  #\n                                 EPILOGUE_SUBTILE: tl.constexpr,  #\n                                 NUM_SMS: tl.constexpr):  #\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    # tile_id_c is used in the epilogue to break the dependency between\n    # the prologue and the epilogue\n    tile_id_c = start_pid - NUM_SMS\n\n    ki = -1\n\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype)\n        b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype)\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n            tile_id_c += NUM_SMS\n            group_id = tile_id_c // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id_c % group_size_m)\n            pid_n = (tile_id_c % num_pid_in_group) // group_size_m\n\n            offs_am_c = pid_m * BLOCK_SIZE_M\n            offs_bn_c = pid_n * BLOCK_SIZE_N\n\n            # Epilogue subtiling is a technique to break our computation and stores into multiple pieces\n            # By subtiling we can reduce shared memory consumption by the epilogue and instead use that\n            # memory to increase our stage count.\n            # In this case we partition the accumulator into 2 BLOCK_SIZE_M x BLOCK_SIZE_N // 2 tensors\n            if EPILOGUE_SUBTILE:\n                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n                acc = tl.permute(acc, (0, 2, 1))\n                acc0, acc1 = tl.split(acc)\n                c0 = acc0.to(dtype)\n                tl._experimental_descriptor_store(c_desc_ptr, c0, [offs_am_c, offs_bn_c])\n                c1 = acc1.to(dtype)\n                tl._experimental_descriptor_store(c_desc_ptr, c1, [offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])\n            else:\n                accumulator = accumulator.to(dtype)\n                tl._experimental_descriptor_store(c_desc_ptr, accumulator, [offs_am_c, offs_bn_c])\n\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_tma_persistent(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n\n    desc_helper = TmaAutoTuneHelper()\n    desc_helper.init_tma_descriptor(\"a\")\n    desc_helper.init_tma_descriptor(\"b\")\n    desc_helper.init_tma_descriptor(\"c\")\n\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n\n    def grid(META):\n        nonlocal desc_helper\n        desc_helper.fill_2d_tma_descriptor(\n            \"a\",\n            a.data_ptr(),\n            M,\n            K,\n            META[\"BLOCK_SIZE_M\"],\n            META[\"BLOCK_SIZE_K\"],\n            a.element_size(),\n        )\n\n        desc_helper.fill_2d_tma_descriptor(\n            \"b\",\n            b.data_ptr(),\n            N,\n            K,\n            META[\"BLOCK_SIZE_N\"],\n            META[\"BLOCK_SIZE_K\"],\n            b.element_size(),\n        )\n\n        store_block_n = META[\"BLOCK_SIZE_N\"]\n\n        if META[\"EPILOGUE_SUBTILE\"]:\n            store_block_n = store_block_n // 2\n\n        desc_helper.fill_2d_tma_descriptor(\n            \"c\",\n            c.data_ptr(),\n            M,\n            N,\n            META[\"BLOCK_SIZE_M\"],\n            store_block_n,\n            c.element_size(),\n        )\n\n        return (min(\n            NUM_SMS,\n            triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n        ), )\n\n    desc_a = desc_helper.get_tma_descriptor_kernel_param(\"a\")\n    desc_b = desc_helper.get_tma_descriptor_kernel_param(\"b\")\n    desc_c = desc_helper.get_tma_descriptor_kernel_param(\"c\")\n\n    matmul_kernel_tma_persistent[grid](\n        desc_a, desc_b, desc_c,  #\n        M, N, K,  #\n        FP8_OUTPUT=dtype == torch.float8_e4m3fn,  #\n        NUM_SMS=NUM_SMS,  #\n    )\n    return c\n\n\n@triton.autotune(\n    configs=matmul_tma_persistent_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_descriptor_persistent(a_ptr, b_ptr, c_ptr,  #\n                                        M, N, K,  #\n                                        BLOCK_SIZE_M: tl.constexpr,  #\n                                        BLOCK_SIZE_N: tl.constexpr,  #\n                                        BLOCK_SIZE_K: tl.constexpr,  #\n                                        GROUP_SIZE_M: tl.constexpr,  #\n                                        EPILOGUE_SUBTILE: tl.constexpr,  #\n                                        NUM_SMS: tl.constexpr):  #\n    # Matmul using TMA and device-side descriptor creation\n    dtype = c_ptr.dtype.element_ty\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    a_desc = tl._experimental_make_tensor_descriptor(\n        a_ptr,\n        shape=[M, K],\n        strides=[K, 1],\n        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],\n    )\n    b_desc = tl._experimental_make_tensor_descriptor(\n        b_ptr,\n        shape=[N, K],\n        strides=[K, 1],\n        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],\n    )\n    c_desc = tl._experimental_make_tensor_descriptor(\n        c_ptr,\n        shape=[M, N],\n        strides=[N, 1],\n        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2],\n    )\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = a_desc.load([offs_am, offs_k])\n        b = b_desc.load([offs_bn, offs_k])\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n\n            if EPILOGUE_SUBTILE:\n                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n                acc = tl.permute(acc, (0, 2, 1))\n                acc0, acc1 = tl.split(acc)\n                c0 = acc0.to(dtype)\n                c_desc.store([offs_am, offs_bn], c0)\n                c1 = acc1.to(dtype)\n                c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)\n            else:\n                c = accumulator.to(dtype)\n                c_desc.store([offs_am, offs_bn], c)\n\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_descriptor_persistent(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n\n    # TMA descriptors require a global memory allocation\n    def alloc_fn(size: int, alignment: int, stream: Optional[int]):\n        return torch.empty(size, device=\"cuda\", dtype=torch.int8)\n\n    triton.set_allocator(alloc_fn)\n\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_descriptor_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        NUM_SMS=NUM_SMS,  #\n    )\n    return c\n\n\ndef cublas_matmul(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    with proton.scope(f\"cublas [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        cublas.matmul(a, b, c)\n    return c\n\n\ndef torch_matmul(a, b):\n    M, K = a.shape\n    N, K = b.shape\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    with proton.scope(f\"torch [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        c = torch.matmul(a, b.T)\n    return c\n\n\n@contextmanager\ndef proton_context():\n    proton.activate(0)\n    try:\n        yield\n    finally:\n        proton.deactivate(0)\n\n\ndef bench_fn(reps, warmup_reps, fn, *args):\n    for _ in range(warmup_reps):\n        fn(*args)\n    with proton_context():\n        for _ in range(reps):\n            fn(*args)\n\n\ndef bench(K, dtype, reps=1000, warmup_reps=10000):\n    M = 8192\n    N = 8192\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n\n    b = b.T.contiguous()\n\n    if cublas is not None:\n        bench_fn(reps, warmup_reps, cublas_matmul, a, b)\n    if dtype == torch.float16:\n        bench_fn(reps, warmup_reps, torch_matmul, a, b)\n    bench_fn(reps, warmup_reps, matmul, a, b.T)\n    bench_fn(reps, warmup_reps, matmul_persistent, a, b.T)\n    if supports_tma():\n        bench_fn(reps, warmup_reps, matmul_tma_persistent, a, b)\n        bench_fn(reps, warmup_reps, matmul_descriptor_persistent, a, b)\n\n\ndef validate(M, N, K, dtype):\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = b.T.contiguous()\n\n    torch_result = torch_matmul(a, b) if dtype == torch.float16 else None\n    cublas_result = cublas_matmul(a, b) if cublas is not None else None\n    naive_result = matmul(a, b.T)\n    persistent_result = matmul_persistent(a, b.T)\n    tma_persistent_result = matmul_tma_persistent(a, b) if supports_tma() else None\n    descriptor_persistent_result = matmul_descriptor_persistent(a, b) if supports_tma() else None\n\n    if torch_result is not None:\n        naive_vs_torch = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), torch_result.to(torch.float16),\n                                               atol=1.0) else \"\u274c\"\n    if cublas_result is not None:\n        naive_vs_cublas = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), cublas_result.to(torch.float16),\n                                                atol=1.0) else \"\u274c\"\n    naive_vs_persistent = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), persistent_result.to(torch.float16),\n                                                atol=1.0) else \"\u274c\"\n    if tma_persistent_result is not None:\n        naive_vs_tma_persistent = \"\u2705\" if torch.allclose(cublas_result.to(torch.float16),\n                                                        tma_persistent_result.to(torch.float16), atol=1.0) else \"\u274c\"\n    if descriptor_persistent_result is not None:\n        naive_vs_descriptor_persistent = \"\u2705\" if torch.allclose(cublas_result.to(\n            torch.float16), descriptor_persistent_result.to(torch.float16), atol=1.0) else \"\u274c\"\n    print(f\"M={M}, N={N}, K={K} verification naive vs: \", end=\"\")\n    if torch_result is not None:\n        print(f\"torch: {naive_vs_torch} \", end=\"\")\n    if cublas_result is not None:\n        print(f\"cublas: {naive_vs_cublas} \", end=\"\")\n    print(f\"persistent: {naive_vs_persistent} \", end=\"\")\n    if tma_persistent_result is not None:\n        print(f\"TMA persistent: {naive_vs_tma_persistent} \", end=\"\")\n    if descriptor_persistent_result is not None:\n        print(f\"Tensor descriptor persistent: {naive_vs_descriptor_persistent} \", end=\"\")\n    print()\n\n\ndef show_profile(precision, profile_name):\n    import triton.profiler.viewer as proton_viewer\n    metrics = [\"time/ms\"]\n    if precision == 'fp8':\n        metrics = [\"tflop8/s\"] + metrics\n    elif precision == 'fp16':\n        metrics = [\"tflop16/s\"] + metrics\n    file_name = f\"{profile_name}.hatchet\"\n    proton_viewer.parse(metrics, file_name, depth=100)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-K\", type=int, required=False, default=512)\n    parser.add_argument(\"--K_range\", type=int, nargs=2)\n    parser.add_argument(\"--K_step\", type=int, default=512)\n    parser.add_argument(\"--prec\", type=str, choices=[\"fp8\", \"fp16\"], default=\"fp16\")\n    args = parser.parse_args()\n\n    if args.prec == 'fp8' and (not hasattr(torch, \"float8_e4m3fn\") or not is_cuda()):\n        print(\"This example requires CUDA with fp8 support.\")\n        exit(1)\n\n    dtype = torch.float8_e4m3fn if args.prec == 'fp8' else torch.float16\n\n    if args.K and args.K_range is None:\n        args.K_range = [args.K, args.K]\n        args.K_step = 1  # doesn't matter as long as it's not 0\n\n    torch.manual_seed(0)\n\n    validate(32, 32, 32, dtype)\n    validate(8192, 8192, args.K_range[0], dtype)\n\n    proton.start(\"matmul\", hook=\"triton\")\n    for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n        bench(K, dtype)\n    proton.finalize()\n    show_profile(args.prec, \"matmul\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}